<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>PointNet网络 | 岁染</title><meta name="keywords" content="深度学习,神经网络,点云"><meta name="author" content="Kagura,651421775@qq.com"><meta name="copyright" content="Kagura"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="PointNet网络详解  一、点云数据的特点  无序性 近密远疏 非结构化数据 局部结构语义  二、PointNet 2.1  PointNet思想 传统的卷积神经网络是对图像像素进行卷积，如果在不同方向将点云数据进行投影，再利用卷积神经网络也能实现分割，但是算法复杂且效果不好。PointNet考虑的是直接输入点云数据，实现一个端到端的网络。 但是点云数据不同于图像数据，首先是点具有置换不变性，">
<meta property="og:type" content="article">
<meta property="og:title" content="PointNet网络">
<meta property="og:url" content="http://example.com/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="岁染">
<meta property="og:description" content="PointNet网络详解  一、点云数据的特点  无序性 近密远疏 非结构化数据 局部结构语义  二、PointNet 2.1  PointNet思想 传统的卷积神经网络是对图像像素进行卷积，如果在不同方向将点云数据进行投影，再利用卷积神经网络也能实现分割，但是算法复杂且效果不好。PointNet考虑的是直接输入点云数据，实现一个端到端的网络。 但是点云数据不同于图像数据，首先是点具有置换不变性，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://w.wallhaven.cc/full/z8/wallhaven-z8dg9y.png">
<meta property="article:published_time" content="2022-08-01T04:46:37.000Z">
<meta property="article:modified_time" content="2022-08-10T15:25:58.195Z">
<meta property="article:author" content="Kagura">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="点云">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://w.wallhaven.cc/full/z8/wallhaven-z8dg9y.png"><link rel="shortcut icon" href="/img/3d.png"><link rel="canonical" href="http://example.com/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PointNet网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-08-10 23:25:58'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/universe.css"><script src="https://npm.elemecdn.com/echarts@4.7.0/dist/echarts.min.js"></script><style type="text/css">#toggle-sidebar {bottom: 80px}</style><style type="text/css">#toggle-sidebar {left:100px}</style><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card/lib/categorybar.css"><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/clock.min.css" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="岁染" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images.jingyeqian.com/i/2021/03/21/6375192146687954551533103.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fas fa-music"></i><span> 番剧</span></a></li><li><a class="site-page child" href="/Diary"><i class="fa-fw fas fa-images"></i><span> 日记</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/Todo-List"><i class="fa-fw fas fa-bell"></i><span> Todo-List</span></a></li><li><a class="site-page child" href="/PDF"><i class="fa-fw fas fa-link"></i><span> PDF</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://w.wallhaven.cc/full/z8/wallhaven-z8dg9y.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">岁染</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fas fa-music"></i><span> 番剧</span></a></li><li><a class="site-page child" href="/Diary"><i class="fa-fw fas fa-images"></i><span> 日记</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/Todo-List"><i class="fa-fw fas fa-bell"></i><span> Todo-List</span></a></li><li><a class="site-page child" href="/PDF"><i class="fa-fw fas fa-link"></i><span> PDF</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PointNet网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-01T04:46:37.000Z" title="发表于 2022-08-01 12:46:37">2022-08-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-10T15:25:58.195Z" title="更新于 2022-08-10 23:25:58">2022-08-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%B8%8D%E6%83%B3%E5%AD%A6%E8%BE%A3/">不想学辣</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PointNet网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>PointNet网络详解</h1>
<hr>
<h2 id="一、点云数据的特点">一、点云数据的特点</h2>
<ul>
<li>无序性</li>
<li>近密远疏</li>
<li>非结构化数据</li>
<li>局部结构语义</li>
</ul>
<h2 id="二、PointNet">二、PointNet</h2>
<h3 id="2-1-PointNet思想">2.1  PointNet思想</h3>
<p>传统的卷积神经网络是对图像像素进行卷积，如果在不同方向将点云数据进行投影，再利用卷积神经网络也能实现分割，但是算法复杂且效果不好。PointNet考虑的是直接输入点云数据，实现一个端到端的网络。</p>
<p>但是点云数据不同于图像数据，首先是点具有<strong>置换不变性</strong>，即交换任意点之间的位置，不会对整体造成影响(不考虑回波和辐射强度时)。</p>
<p>PointNet需要满足这种不变性，即:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>≡</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>π</mi><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>π</mi><mn>2</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>π</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x_1,x_2,...,x_n)\equiv f(x_{\pi1},x_{\pi2},...,x_{\pi n})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≡</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">πn</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其实有很多种方式能满足这种不变性，PointNet采用了最大池化(max函数)来实现。但是该方法太过一刀切了，会丢失很多的信息。为了确保信息量，PointNet采用升维的方式，构建更多的隐含信息。</p>
<p>最简单的就是通过全连接层或者卷积进行特征提取。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/image-20220328001047560.png" alt="image-20220328001047560" style="zoom:33%;">
<p>其网络架构如下：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/image-20220328124916753.png" alt="image-20220328124916753" style="zoom: 33%;">
<p>大致流程为：</p>
<ul>
<li>输入一个包含n个点云的集合，表示为<code>n*3</code>的<code>tensor</code>，三个维度分别对应<code>xyz</code>坐标。</li>
<li>输入的数据一般需要跟一个<code>T-Net</code>学习到的转移矩阵相乘来对其，这样保证了模型的对特定空间转换的不变性。</li>
<li>最终利用<code>maxpooling</code>在各个维度上操作得到全局特征。</li>
</ul>
<p>对于分类工作，是将输入数据先做一个数据增强，然后利用多层感知机进行升维，在获取到1024维的信息之后，采用最大池化获取全局信息。接着在对这1024维进行降维到k维，进行分类。</p>
<p>而针对分割任务，则是将全局高维信息与64维的低维信息融合(等于64维后面直接黏上1024维)后做降维。</p>
<h3 id="2-2-PointNet实现">2.2  PointNet实现</h3>
<p>本文代码参考自：<a href="https://link.zhihu.com/?target=https%3A//github.com/yanx27/Pointnet_Pointnet2_pytorch">https://link.zhihu.com/?target=https%3A//github.com/yanx27/Pointnet_Pointnet2_pytorch</a>.</p>
<h4 id="2-2-1-T-Net">2.2.1  <strong>T-Net</strong></h4>
<p>T-Net是用来模拟模型对特定空间转换的不变性，在原文中给出了如下的解释：<em>The semantic labeling of a point cloud has to be invariant if the point cloud undergoes certain geometric transformations, such as rigid transformation. We therefore expect that the learnt representation by our point set is invariant to these transformations.</em></p>
<p>本质上是做了<strong>刚体变换</strong>(Rigid Transformation)，即变换前后两点间距离仍保持不变。具体原理可以参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/xinxue/archive/2017/09/28/7513192.html">文章</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">STN3d</span>(nn.Module):</span><br><span class="line">    <span class="comment"># T-Net在三维情况下</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,channel</span>):</span><br><span class="line">        <span class="built_in">super</span>(STN3d, self).__init__()</span><br><span class="line">        self.conv1=nn.Conv1d(channel,<span class="number">64</span>,<span class="number">1</span>)</span><br><span class="line">        self.conv2=nn.Conv1d(<span class="number">64</span>,<span class="number">128</span>,<span class="number">1</span>)</span><br><span class="line">        self.conv3=nn.Conv1d(<span class="number">128</span>,<span class="number">1024</span>,<span class="number">1</span>)</span><br><span class="line">        self.fc1=nn.Linear(<span class="number">1024</span>,<span class="number">512</span>)</span><br><span class="line">        self.fc2=nn.Linear(<span class="number">512</span>,<span class="number">256</span>)</span><br><span class="line">        <span class="comment"># 这里的9是3*3变换来的</span></span><br><span class="line">        self.fc3=nn.Linear(<span class="number">256</span>,<span class="number">9</span>)</span><br><span class="line">        self.relu=nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.bn1=nn.BatchNorm1d(<span class="number">64</span>)</span><br><span class="line">        self.bn2=nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line">        self.bn3=nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.bn4=nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.bn5=nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        batchsize=x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始获取高维数据</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape: [ batch , num , 3 ]</span></span><br><span class="line">        x=F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        <span class="comment"># shape: [ batch , num , 64 ]</span></span><br><span class="line">        x=F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        <span class="comment"># shape: [ batch , num , 128 ]</span></span><br><span class="line">        x=F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        <span class="comment"># shape: [ batch , num , 1024 ]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最大池化获取全局信息</span></span><br><span class="line">        x=torch.<span class="built_in">max</span>(x,<span class="number">2</span>,keepdim=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 展平做线性层</span></span><br><span class="line">        x=x.view(-<span class="number">1</span>,<span class="number">1024</span>)</span><br><span class="line">        <span class="comment"># shape: [ 1, 1024 ]</span></span><br><span class="line">        x=F.relu(self.bn4(self.fc1(x)))</span><br><span class="line">        <span class="comment"># shape: [ 1, 512 ]</span></span><br><span class="line">        x=F.relu(self.bn5(self.fc2(x)))</span><br><span class="line">        <span class="comment"># shape: [ 1, 256 ]</span></span><br><span class="line">        x=self.fc3(x)</span><br><span class="line">        <span class="comment"># shape: [ 1, 9 ]</span></span><br><span class="line">        <span class="comment"># 原本的三维xyz变换到了9维(3*3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 关于iden，这东西就是一个eyes矩阵，本质上相当于给变换的结果加上input本身</span></span><br><span class="line">        iden=Variable(torch.from_numpy(np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]).astype(np.float32)))\</span><br><span class="line">            .view(<span class="number">1</span>,<span class="number">9</span>).repeat(batchsize,<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># shape: [ batch , 9 ]</span></span><br><span class="line">        <span class="keyword">if</span> x.is_cuda:</span><br><span class="line">            iden=iden.cuda()</span><br><span class="line"></span><br><span class="line">        x+=iden</span><br><span class="line">        <span class="comment"># 转换成[ batch , 3 , 3 ]的矩阵进行输出</span></span><br><span class="line">        <span class="comment"># 该矩阵用于对原始向量做刚体变换</span></span><br><span class="line">        x=x.view(-<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/image-20220328200404491.png" alt="image-20220328200404491" style="zoom:50%;">
<p><code>T-Net</code>就相当于一个微型网络，能够获得一个用于变换的数据，且该数据是能自适应的。</p>
<p>输入数据如果是一个<code>3*1000</code>的点云，得到的网络结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (<span class="built_in">type</span>)               Output Shape         Param <span class="comment">#</span></span><br><span class="line">================================================================</span><br><span class="line">            Conv1d-<span class="number">1</span>             [-<span class="number">1</span>, <span class="number">64</span>, <span class="number">1000</span>]             <span class="number">256</span></span><br><span class="line">       BatchNorm1d-<span class="number">2</span>             [-<span class="number">1</span>, <span class="number">64</span>, <span class="number">1000</span>]             <span class="number">128</span></span><br><span class="line">            Conv1d-<span class="number">3</span>            [-<span class="number">1</span>, <span class="number">128</span>, <span class="number">1000</span>]           <span class="number">8</span>,<span class="number">320</span></span><br><span class="line">       BatchNorm1d-<span class="number">4</span>            [-<span class="number">1</span>, <span class="number">128</span>, <span class="number">1000</span>]             <span class="number">256</span></span><br><span class="line">            Conv1d-<span class="number">5</span>           [-<span class="number">1</span>, <span class="number">1024</span>, <span class="number">1000</span>]         <span class="number">132</span>,096</span><br><span class="line">       BatchNorm1d-<span class="number">6</span>           [-<span class="number">1</span>, <span class="number">1024</span>, <span class="number">1000</span>]           <span class="number">2</span>,048</span><br><span class="line">            Linear-<span class="number">7</span>                  [-<span class="number">1</span>, <span class="number">512</span>]         <span class="number">524</span>,<span class="number">800</span></span><br><span class="line">       BatchNorm1d-<span class="number">8</span>                  [-<span class="number">1</span>, <span class="number">512</span>]           <span class="number">1</span>,024</span><br><span class="line">            Linear-<span class="number">9</span>                  [-<span class="number">1</span>, <span class="number">256</span>]         <span class="number">131</span>,<span class="number">328</span></span><br><span class="line">      BatchNorm1d-<span class="number">10</span>                  [-<span class="number">1</span>, <span class="number">256</span>]             <span class="number">512</span></span><br><span class="line">           Linear-<span class="number">11</span>                    [-<span class="number">1</span>, <span class="number">9</span>]           <span class="number">2</span>,<span class="number">313</span></span><br><span class="line">================================================================</span><br><span class="line">Total params: <span class="number">803</span>,081</span><br><span class="line">Trainable params: <span class="number">803</span>,081</span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): <span class="number">0.01</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">18.57</span></span><br><span class="line">Params size (MB): <span class="number">3.06</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">21.64</span></span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<p>扩展到<code>k</code>维，则是表示如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">STNkd</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(STNkd, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv1d(k, <span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv1d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3 = torch.nn.Conv1d(<span class="number">128</span>, <span class="number">1024</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">256</span>, k * k)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.bn1 = nn.BatchNorm1d(<span class="number">64</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.bn4 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.bn5 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.k = k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batchsize = x.size()[<span class="number">0</span>]</span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        x = torch.<span class="built_in">max</span>(x, <span class="number">2</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.bn4(self.fc1(x)))</span><br><span class="line">        x = F.relu(self.bn5(self.fc2(x)))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line"></span><br><span class="line">        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(<span class="number">1</span>, self.k * self.k).repeat(</span><br><span class="line">            batchsize, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> x.is_cuda:</span><br><span class="line">            iden = iden.cuda()</span><br><span class="line">        x = x + iden</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.k, self.k)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/image-20220328200431913.png" alt="image-20220328200431913"></p>
<h4 id="2-2-2-PointNet">2.2.2  <strong>PointNet</strong></h4>
<p>实现起来也相对简单，与<code>T-Net</code>的差别其实不大。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PointNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,global_feat=<span class="literal">True</span>,feature_transform=<span class="literal">False</span>,channel=<span class="number">3</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param global_feat: 是否返回全局特征,该值为False的时候会返回拼接的信息</span></span><br><span class="line"><span class="string">        :param feature_transform: 要素转换阶段，是否要进行要素转换</span></span><br><span class="line"><span class="string">        :param channel: 输入数据的维度，默认是只含有xyz坐标</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.stn=STN3d(channel)</span><br><span class="line">        self.conv1 = torch.nn.Conv1d(channel, <span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv1d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3 = torch.nn.Conv1d(<span class="number">128</span>, <span class="number">1024</span>, <span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(<span class="number">64</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.global_feat=global_feat</span><br><span class="line">        self.feature_transform=feature_transform</span><br><span class="line">        <span class="keyword">if</span> self.feature_transform:</span><br><span class="line">            self.fstn=STNkd(k=<span class="number">64</span>)</span><br><span class="line">        </span><br><span class="line">        self.mlp1=nn.Linear(<span class="number">1024</span>,<span class="number">128</span>)</span><br><span class="line">        self.mlp2=nn.Linear(<span class="number">128</span>,<span class="number">64</span>)</span><br><span class="line">        self.mlp3=nn.Linear(<span class="number">64</span>,n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        B, D, N=x.size() <span class="comment"># batch , deep , num</span></span><br><span class="line">        trans=self.stn(x) <span class="comment"># return : shape: [ b , 3 , 3 ]</span></span><br><span class="line">        x=x.transpose(<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># [ b , num , deep ]</span></span><br><span class="line">        <span class="keyword">if</span> D&gt;<span class="number">3</span>:</span><br><span class="line">            <span class="comment"># 此时需要分割要素</span></span><br><span class="line">            <span class="comment"># 我们做刚体变换的只有位置数据</span></span><br><span class="line">            x,feature=x.split(<span class="number">3</span>,dim=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 此时做矩阵乘法，bmm这个方法一定要三维才能进行</span></span><br><span class="line">        <span class="comment"># 相当于[x&#x27;,y&#x27;,z&#x27;]+[x,y,z]</span></span><br><span class="line">        <span class="comment"># [x&#x27;,y&#x27;,z&#x27;]来自于trans矩阵的变换</span></span><br><span class="line">        x=torch.bmm(x,trans) <span class="comment"># [ num , deep ] * [ deep , deep ]-&gt; [ num , deep ]</span></span><br><span class="line">        <span class="keyword">if</span> D&gt;<span class="number">3</span>:</span><br><span class="line">            x=torch.cat([x,feature],dim=<span class="number">2</span>)</span><br><span class="line">        x.transpose(<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># [ b , d , n ]</span></span><br><span class="line"></span><br><span class="line">        x=F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        <span class="comment"># 64个特征时是否需要做feature_transform</span></span><br><span class="line">        <span class="keyword">if</span> self.feature_transform:</span><br><span class="line">            trans_feat=self.fstn(x)</span><br><span class="line">            x=x.transpose(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">            x=torch.bmm(x,trans_feat)</span><br><span class="line">            x=x.transpose(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_feat=<span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 此时若是处理分割任务，则将该部分(64维特征)作为拼接项</span></span><br><span class="line">        pointfeat=x</span><br><span class="line">        <span class="comment"># 接着进行卷积</span></span><br><span class="line">        x=F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x=self.bn3(self.conv3(x))</span><br><span class="line">        x=torch.argmax(x,<span class="number">2</span>,keepdim=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        x=x.view(-<span class="number">1</span>,<span class="number">1024</span>)</span><br><span class="line">        <span class="keyword">if</span> self.global_feat:</span><br><span class="line">            <span class="comment"># 分类任务</span></span><br><span class="line">            <span class="comment"># trans是input_transform的3*3矩阵</span></span><br><span class="line">            <span class="comment"># trans是feature_transform的64*64矩阵</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 为了方便演示，在编码器上加上了解码器的工作</span></span><br><span class="line">            <span class="comment"># 实际上没有下面三行</span></span><br><span class="line">            x=F.relu(self.bn2(self.mlp1(x)))</span><br><span class="line">            x=F.relu(self.bn1(self.mlp2(x)))</span><br><span class="line">            x=self.mlp3(x)</span><br><span class="line">            <span class="keyword">return</span> x,trans,trans_feat</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 分割任务，需要将全局信息黏贴到中间层信息中</span></span><br><span class="line">            x=x.view(-<span class="number">1</span>,<span class="number">1024</span>,<span class="number">1</span>).repeat(<span class="number">1</span>,<span class="number">1</span>,N)</span><br><span class="line">            <span class="keyword">return</span> torch.cat([x,pointfeat],<span class="number">1</span>),trans,trans_feat</span><br></pre></td></tr></table></figure>
<p>论文中提到，64*64维的矩阵很难优化，但作者发现如果该矩阵约等于正交矩阵，优化就会容易很多。根据正交矩阵的性质：正交矩阵乘以转置等于单位矩阵，作者额外增加了损失函数。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub><mo>=</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>I</mi><mo>−</mo><mi>A</mi><msup><mi>A</mi><mi>T</mi></msup><mi mathvariant="normal">∣</mi><msubsup><mi mathvariant="normal">∣</mi><mi>F</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">L_{reg}=||I-AA^T||^2_F
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>是通过<code>T-Net</code>得到的<code>64*64</code>对齐矩阵，在本部分中，作者给出的损失函数代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feature_transform_reguliarzer</span>(<span class="params">trans</span>):</span><br><span class="line">    <span class="comment"># 定义损失规则</span></span><br><span class="line">    d = trans.size()[<span class="number">1</span>] <span class="comment"># deep</span></span><br><span class="line">    I = torch.eye(d)[<span class="literal">None</span>, :, :] <span class="comment"># [ 1 , deep , deep ]</span></span><br><span class="line">    <span class="keyword">if</span> trans.is_cuda:</span><br><span class="line">        I = I.cuda()</span><br><span class="line">    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(<span class="number">2</span>, <span class="number">1</span>) - I), dim=(<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 虽然但是...按照公式写应该是</span></span><br><span class="line">    <span class="comment">#  loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1))- I, dim=(1, 2)))</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>话说F范数就是对向量的所有元素平方求和再开方，本质上是向量模的度量。如果向量内的数据都是无量纲的，那么开不开方影响就不大了。</p>
<p>我们可以查看下网络的结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (<span class="built_in">type</span>)               Output Shape         Param <span class="comment">#</span></span><br><span class="line">================================================================</span><br><span class="line">            Conv1d-<span class="number">1</span>             [-<span class="number">1</span>, <span class="number">64</span>, <span class="number">1000</span>]             <span class="number">256</span></span><br><span class="line">       BatchNorm1d-<span class="number">2</span>             [-<span class="number">1</span>, <span class="number">64</span>, <span class="number">1000</span>]             <span class="number">128</span></span><br><span class="line">            Conv1d-<span class="number">3</span>            [-<span class="number">1</span>, <span class="number">128</span>, <span class="number">1000</span>]           <span class="number">8</span>,<span class="number">320</span></span><br><span class="line">       BatchNorm1d-<span class="number">4</span>            [-<span class="number">1</span>, <span class="number">128</span>, <span class="number">1000</span>]             <span class="number">256</span></span><br><span class="line">            Conv1d-<span class="number">5</span>           [-<span class="number">1</span>, <span class="number">1024</span>, <span class="number">1000</span>]         <span class="number">132</span>,096</span><br><span class="line">       BatchNorm1d-<span class="number">6</span>           [-<span class="number">1</span>, <span class="number">1024</span>, <span class="number">1000</span>]           <span class="number">2</span>,048</span><br><span class="line">            Linear-<span class="number">7</span>                  [-<span class="number">1</span>, <span class="number">512</span>]         <span class="number">524</span>,<span class="number">800</span></span><br><span class="line">       BatchNorm1d-<span class="number">8</span>                  [-<span class="number">1</span>, <span class="number">512</span>]           <span class="number">1</span>,024</span><br><span class="line">            Linear-<span class="number">9</span>                  [-<span class="number">1</span>, <span class="number">256</span>]         <span class="number">131</span>,<span class="number">328</span></span><br><span class="line">      BatchNorm1d-<span class="number">10</span>                  [-<span class="number">1</span>, <span class="number">256</span>]             <span class="number">512</span></span><br><span class="line">           Linear-<span class="number">11</span>                    [-<span class="number">1</span>, <span class="number">9</span>]           <span class="number">2</span>,<span class="number">313</span></span><br><span class="line">            STN3d-<span class="number">12</span>                 [-<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]               <span class="number">0</span></span><br><span class="line">           Conv1d-<span class="number">13</span>             [-<span class="number">1</span>, <span class="number">64</span>, <span class="number">1000</span>]             <span class="number">256</span></span><br><span class="line">      BatchNorm1d-<span class="number">14</span>             [-<span class="number">1</span>, <span class="number">64</span>, <span class="number">1000</span>]             <span class="number">128</span></span><br><span class="line">           Conv1d-<span class="number">15</span>            [-<span class="number">1</span>, <span class="number">128</span>, <span class="number">1000</span>]           <span class="number">8</span>,<span class="number">320</span></span><br><span class="line">      BatchNorm1d-<span class="number">16</span>            [-<span class="number">1</span>, <span class="number">128</span>, <span class="number">1000</span>]             <span class="number">256</span></span><br><span class="line">           Conv1d-<span class="number">17</span>           [-<span class="number">1</span>, <span class="number">1024</span>, <span class="number">1000</span>]         <span class="number">132</span>,096</span><br><span class="line">      BatchNorm1d-<span class="number">18</span>           [-<span class="number">1</span>, <span class="number">1024</span>, <span class="number">1000</span>]           <span class="number">2</span>,048</span><br><span class="line">================================================================</span><br><span class="line">Total params: <span class="number">946</span>,<span class="number">185</span></span><br><span class="line">Trainable params: <span class="number">946</span>,<span class="number">185</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): <span class="number">0.01</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">37.12</span></span><br><span class="line">Params size (MB): <span class="number">3.61</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">40.74</span></span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h4 id="2-2-3-数据加载">2.2.3  <strong>数据加载</strong></h4>
<p>这里我们随便测试下数据，加载器就随便写写了</p>
<p>注意Model是刚刚定义的模型文件夹，pointnet是写PointNet的.py文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> Model <span class="keyword">import</span> pointnet</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>我们就单独取一个类试试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">path=<span class="string">r&quot;.modelnet40_normal_resampled\car\\&quot;</span></span><br><span class="line">file_list=os.listdir(path)</span><br></pre></td></tr></table></figure>
<p>接着定义数据读取方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个数据读取类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PCDataset</span>(Data.Dataset):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,file_list</span>):</span><br><span class="line">        self.file_list=file_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path + file_list[idx]) <span class="keyword">as</span> f:</span><br><span class="line">            data = f.readlines()</span><br><span class="line">        data = [i.split(<span class="string">&quot;\n&quot;</span>)[<span class="number">0</span>].split(<span class="string">&quot;,&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">        data=np.array(data,dtype=<span class="built_in">float</span>)</span><br><span class="line">        <span class="keyword">return</span> data,<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">Dataset=PCDataset(file_list)</span><br><span class="line">train_loader=Data.DataLoader(</span><br><span class="line">    dataset=Dataset,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    batch_size=<span class="number">9</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>网络搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建PointNet</span></span><br><span class="line">pn=pointnet.PointNet(<span class="number">3</span>)</span><br><span class="line">criterion=nn.NLLLoss()</span><br><span class="line">optimizer=torch.optim.Adam(pn.parameters(),lr=<span class="number">0.0003</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单测试下</span></span><br><span class="line">loss_list=[]</span><br><span class="line">acc_list=[]</span><br><span class="line"></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">pn.to(device)</span><br><span class="line">criterion.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step,(bx,by) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    bx,by=bx.to(device),by.to(device)</span><br><span class="line">    out = pn(bx.to(torch.float32))[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Result&#x27;s Size&quot;</span>, out.shape)</span><br><span class="line">    out = F.softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    pre_lab = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;class&quot;</span>, pre_lab)</span><br><span class="line">    <span class="built_in">print</span>(by)</span><br><span class="line">    loss = criterion(out, by)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line">    acc_list.append(accuracy_score(pre_lab,by))</span><br></pre></td></tr></table></figure>
<p>结果可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(train_loader)),loss_list,<span class="string">&quot;ro-&quot;</span>,label=<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(train_loader)),acc_list,<span class="string">&quot;bs-&quot;</span>,label=<span class="string">&quot;acc&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/image-20220329152205605.png" alt="image-20220329152205605" style="zoom: 33%;">
<p>损失函数出现负值是因为用的函数是NLLoss，然后小batch的训练其实没多大意义，这里只是测试下。</p>
<h3 id="2-3-PointNet缺点">2.3  PointNet缺点</h3>
<ul>
<li>PointNet与当下主流网络不符，只是做了全局信息的融合，并没有考虑到局部的语义</li>
<li>点对之间的特征关系并没有考虑</li>
</ul>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E7%82%B9%E4%BA%91/">点云</a></div><div class="post_share"><div class="social-share" data-image="https://w.wallhaven.cc/full/z8/wallhaven-z8dg9y.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BF%AB%E9%80%9F%E5%9B%BE%E5%83%8F%E9%A2%9C%E8%89%B2%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/ca60c301082c4f1c875ee3d17b8cba6a.jpeg#pic_center" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">快速图像颜色风格迁移</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet-%E7%BD%91%E7%BB%9C/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://w.wallhaven.cc/full/z8/wallhaven-z8dg9y.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PointNet++网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet-%E7%BD%91%E7%BB%9C/" title="PointNet++网络"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://w.wallhaven.cc/full/z8/wallhaven-z8dg9y.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-01</div><div class="title">PointNet++网络</div></div></a></div><div><a href="/2022/07/29/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="图卷积神经网络(GCN)综述与实现"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://w.wallhaven.cc/full/y8/wallhaven-y8vlyk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-29</div><div class="title">图卷积神经网络(GCN)综述与实现</div></div></a></div><div><a href="/2022/07/29/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91FCN%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C/" title="FCN语义分割网络"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/ca60c301082c4f1c875ee3d17b8cba6a.jpeg#pic_center" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-29</div><div class="title">FCN语义分割网络</div></div></a></div><div><a href="/2022/07/28/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AC%AC%E4%BA%94%E7%AB%A0%20%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="第五章  全连接神经网络"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/ca60c301082c4f1c875ee3d17b8cba6a.jpeg#pic_center" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-28</div><div class="title">第五章  全连接神经网络</div></div></a></div><div><a href="/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BF%AB%E9%80%9F%E5%9B%BE%E5%83%8F%E9%A2%9C%E8%89%B2%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/" title="快速图像颜色风格迁移"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/ca60c301082c4f1c875ee3d17b8cba6a.jpeg#pic_center" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-01</div><div class="title">快速图像颜色风格迁移</div></div></a></div><div><a href="/2022/07/28/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AC%AC%E4%B8%80%E7%AB%A0%20%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8EPyTorch/" title="第一章  深度学习与PyTorch"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/ca60c301082c4f1c875ee3d17b8cba6a.jpeg#pic_center" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-28</div><div class="title">第一章  深度学习与PyTorch</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images.jingyeqian.com/i/2021/03/21/6375192146687954551533103.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Kagura</div><div class="author-info__description">May we all be who we want to be</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45957458?type=blog"><i></i><span>about me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xxxxx" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><script src="https://fastly.jsdelivr.net/gh/xiaopengand/blogCdn@latest/xzxr/twopeople1.js"></script><script src="https://fastly.jsdelivr.net/gh/xiaopengand/blogCdn@latest/xzxr/zdog.dist.js"></script><script id="rendered-js" src="https://fastly.jsdelivr.net/gh/xiaopengand/blogCdn@latest/xzxr/twopeople.js"></script><style>.card-widget.card-announcement {
margin: 0;
align-items: center;
justify-content: center;
text-align: center;
}
canvas {
display: block;
margin: 0 auto;
cursor: move;
}</style><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">PointNet网络详解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">1.1.</span> <span class="toc-text">一、点云数据的特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81PointNet"><span class="toc-number">1.2.</span> <span class="toc-text">二、PointNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-PointNet%E6%80%9D%E6%83%B3"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1  PointNet思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-PointNet%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2  PointNet实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-T-Net"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">2.2.1  T-Net</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-PointNet"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2.2.2  PointNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">2.2.3  数据加载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-PointNet%E7%BC%BA%E7%82%B9"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3  PointNet缺点</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/09/07/%E3%80%90Python%E3%80%91Python%E5%A5%87%E6%80%AA%E7%9A%84%E7%9F%A5%E8%AF%86/" title="Python 奇怪的知识"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/20200718094916304.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 奇怪的知识"/></a><div class="content"><a class="title" href="/2022/09/07/%E3%80%90Python%E3%80%91Python%E5%A5%87%E6%80%AA%E7%9A%84%E7%9F%A5%E8%AF%86/" title="Python 奇怪的知识">Python 奇怪的知识</a><time datetime="2022-09-07T13:43:14.992Z" title="发表于 2022-09-07 21:43:14">2022-09-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/07/%E3%80%90%E7%9F%A5%E4%B9%8E%E8%BD%AC%E8%BD%BD%E3%80%91%E9%BC%BB%E7%82%8E/" title="【知乎转载】鼻炎成因及疗法"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://w.wallhaven.cc/full/6o/wallhaven-6ozkzl.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【知乎转载】鼻炎成因及疗法"/></a><div class="content"><a class="title" href="/2022/09/07/%E3%80%90%E7%9F%A5%E4%B9%8E%E8%BD%AC%E8%BD%BD%E3%80%91%E9%BC%BB%E7%82%8E/" title="【知乎转载】鼻炎成因及疗法">【知乎转载】鼻炎成因及疗法</a><time datetime="2022-09-07T12:35:06.647Z" title="发表于 2022-09-07 20:35:06">2022-09-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/07/%E3%80%90Artist%E3%80%91%E4%BD%9C%E5%93%81%E9%9B%86/" title="【作品集】岁月拾珠"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://w.wallhaven.cc/full/j3/wallhaven-j3m8y5.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【作品集】岁月拾珠"/></a><div class="content"><a class="title" href="/2022/09/07/%E3%80%90Artist%E3%80%91%E4%BD%9C%E5%93%81%E9%9B%86/" title="【作品集】岁月拾珠">【作品集】岁月拾珠</a><time datetime="2022-09-07T10:36:57.526Z" title="发表于 2022-09-07 18:36:57">2022-09-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/07/%E3%80%90%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%91Numpy%E5%85%A5%E9%97%A8/" title="Numpy教程"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://w.wallhaven.cc/full/l3/wallhaven-l3xk6q.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Numpy教程"/></a><div class="content"><a class="title" href="/2022/09/07/%E3%80%90%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%91Numpy%E5%85%A5%E9%97%A8/" title="Numpy教程">Numpy教程</a><time datetime="2022-09-07T05:09:27.030Z" title="发表于 2022-09-07 13:09:27">2022-09-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/06/%E3%80%90%E8%8B%B1%E8%AF%AD%E3%80%91%E8%8B%B1%E8%AF%AD%E5%8F%A3%E8%AF%AD%E5%B8%B8%E7%94%A8%E8%A1%A8%E8%BF%B0/" title="英语口语常用表述"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cloud.miiiku.xyz/src/images/cover/cover-02.jpg-webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="英语口语常用表述"/></a><div class="content"><a class="title" href="/2022/09/06/%E3%80%90%E8%8B%B1%E8%AF%AD%E3%80%91%E8%8B%B1%E8%AF%AD%E5%8F%A3%E8%AF%AD%E5%B8%B8%E7%94%A8%E8%A1%A8%E8%BF%B0/" title="英语口语常用表述">英语口语常用表述</a><time datetime="2022-09-06T04:41:01.000Z" title="发表于 2022-09-06 12:41:01">2022-09-06</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://w.wallhaven.cc/full/z8/wallhaven-z8dg9y.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 By Kagura</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">All amazing things you see have been tempered by mediocrity</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://example.com/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/'
    this.page.identifier = '/2022/08/01/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91PointNet%E7%BD%91%E7%BB%9C/'
    this.page.title = 'PointNet网络'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Valine' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script data-pjax defer src="https://npm.elemecdn.com/tzy-blog/lib/js/theme/chocolate.js"></script><script data-pjax defer src="/js/fish.js"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script async src="/js/diytitle.js"></script><div class="aplayer no-destroy" data-id="7401064131" data-server="tencent" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="255,255,255" opacity="0.8" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>function history_calendar_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height: 100px;overflow: hidden"><div class="history_swiper-container" id="history-container" style="width: 100%;height: 100%"><div class="swiper-wrapper" id="history_container_wrapper" style="height:20px"></div></div></div></div>';
                console.log('已挂载history_calendar')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            history_calendar_injector_config()
        } </script><script data-pjax  src="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.js"></script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 190px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 160px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/akilar-candyassets/image/cover1.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/不想学辣/&quot;);" href="javascript:void(0);">不想学辣</a><span class="categoryBar-list-count">47</span><span class="categoryBar-list-descr">摸鱼人的摸鱼日常</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/akilar-candyassets/image/cover2.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/琐碎日常/&quot;);" href="javascript:void(0);">琐碎日常</a><span class="categoryBar-list-count">4</span><span class="categoryBar-list-descr">生活虐我千百遍</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/akilar-candyassets/image/cover3.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/拾枝杂谈/&quot;);" href="javascript:void(0);">拾枝杂谈</a><span class="categoryBar-list-count">4</span><span class="categoryBar-list-descr">嘘，梦醒了可是要被吃掉的哦</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/akilar-candyassets/image/cover4.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/梦时风月/&quot;);" href="javascript:void(0);">梦时风月</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">可能有用的小知识</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><script data-pjax>
  function butterfly_clock_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/theme_f/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_injector_config();
  }
  </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax src="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/clock.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":20,"vOffset":-20},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>